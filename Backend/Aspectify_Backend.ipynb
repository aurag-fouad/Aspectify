{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06dc053a",
   "metadata": {},
   "source": [
    "# 0- Donwload Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66837f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are in Kaggle\n",
    "# %%capture\n",
    "# !mamba install --force-reinstall aiohttp -y\n",
    "# !pip install -U \"xformers<0.0.26\" --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "# # Temporary fix for https://github.com/huggingface/datasets/issues/6753\n",
    "# !pip install datasets==2.16.0 fsspec==2023.10.0 gcsfs==2023.10.0\n",
    "\n",
    "# import os\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b7ca8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%capture` not found.\n"
     ]
    }
   ],
   "source": [
    "# If you are in Colab\n",
    "%%capture\n",
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "# We have to check which Torch version for Xformers (2.3 -> 0.0.27)\n",
    "from torch import __version__; from packaging.version import Version as V\n",
    "xformers = \"xformers==0.0.27\" if V(__version__) < V(\"2.4.0\") else \"xformers\"\n",
    "!pip install --no-deps {xformers} trl peft accelerate bitsandbytes triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c9825d",
   "metadata": {},
   "source": [
    "# 1- Importation des bibliotheque necessaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c937433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify, session \n",
    "from flask_cors import CORS\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbc4063",
   "metadata": {},
   "source": [
    "#### on va utiliser flask comme un framework de backend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5462c57a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<flask_cors.extension.CORS at 0x293920c7df0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "CORS(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6c8c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "restaurants_aspect_model, restaurants_aspect_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"auragFouad/mistral7b_aspect_restaurants\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(restaurants_aspect_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94518d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops_aspect_model, laptops_aspect_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"auragFouad/mistral7b_aspect_laptops\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(laptops_aspect_model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17996fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants_sentiment_model, restaurants_sentiment_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"auragFouad/llama3_sentiment_restaurants\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(restaurants_sentiment_model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62490fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops_sentiment_model, laptops_sentiment_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"auragFouad/llama3_sentiment_laptops\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(laptops_sentiment_model) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d500fb7a",
   "metadata": {},
   "source": [
    "# Now we define the alpaca prompt and instructions for aspect prediction and sentiment classification on Restaurants and Laptops domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854b30ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61889a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants_aspect_instruction_prompt = \"Recognize all aspect terms in the given review delimited by triple quotes. The aspect terms are nouns or phrases appearing in the review that indicate specific aspects or features of the product/service. choose from the following aspects [RESTAURANT#GENERAL,SERVICE#GENERAL,FOOD#QUALITY,FOOD#STYLE_OPTIONS,DRINKS#STYLE_OPTIONS,DRINKS#PRICES,RESTAURANT#PRICES,RESTAURANT#MISCELLANEOUS,AMBIENCE#GENERAL,FOOD#PRICES,LOCATION#GENERAL,DRINKS#QUALITY]. Answer in the format [aspect] without any explanation, in case of multiple aspects append them into the list of aspects as a list like this [aspect_1, aspect_2]. If no aspect term exists, then only answer [].\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186f61ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops_aspect_instruction_prompt = \"Recognize all aspect terms in the given review delimited by triple quotes. The aspect terms are nouns or phrases appearing in the review that indicate specific aspects or features of the product/service. choose from the following aspects [RESTAURANT#GENERAL,SERVICE#GENERAL,FOOD#QUALITY,FOOD#STYLE_OPTIONS,DRINKS#STYLE_OPTIONS,DRINKS#PRICES,RESTAURANT#PRICES,RESTAURANT#MISCELLANEOUS,AMBIENCE#GENERAL,FOOD#PRICES,LOCATION#GENERAL,DRINKS#QUALITY]. Answer in the format [aspect] without any explanation, in case of multiple aspects append them into the list of aspects as a list like this [aspect_1, aspect_2]. If no aspect term exists, then only answer [].\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8213dabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants_sentiment_instruction_prompt = \"Given a sentence delimited by triple quotes between \\\"\\\" with a specific aspect next to it in between \\\"\\\", predict the sentiment polarity (positive, negative, or neutral) associated with that aspect in the given sentence. Answer back with the sentiment without any explanation.\", # instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907b60b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops_sentiment_instruction_prompt = \"Given a sentence delimited by triple quotes between \\\"\\\" with a specific aspect next to it in between \\\"\\\", predict the sentiment polarity (positive, negative, or neutral) associated with that aspect in the given sentence. Answer back with the sentiment without any explanation.\", # instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146d70e5",
   "metadata": {},
   "source": [
    "## Now  i have 4 models:\n",
    "\n",
    "    - aspect_restaurants_mistral_7b\n",
    "    - aspect_laptop_mistral_7b\n",
    "    - sentiment_restaurants_llama3_8b\n",
    "    - sentiment_laptop_llama3_8b\n",
    "    \n",
    "For each task/domain we define a function for inference, and another for preprocessing the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a91d4ae",
   "metadata": {},
   "source": [
    "# Aspect/Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025905dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference with restaurants_aspect_model/restaurants_aspect_tokenizer/restaurants_aspect_instruction_prompt\n",
    "def get_aspect_restaurants(user_description):\n",
    "    input_text = user_description\n",
    "    inputs = restaurants_aspect_tokenizer(\n",
    "    [\n",
    "      alpaca_prompt.format(\n",
    "          restaurants_aspect_instruction_prompt,\n",
    "          input_text, # input\n",
    "          \"\", # output - leave this blank for generation!\n",
    "      )\n",
    "    ], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = laptops_aspect_model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "    output = restaurants_aspect_tokenizer.batch_decode(outputs)\n",
    "    return output\n",
    "\n",
    "# Preprocessing the model output\n",
    "def get_aspect_restaurants_list(output):\n",
    "    \n",
    "    prediction = output\n",
    "    prediction = prediction[0]\n",
    "    prediction = prediction.split(\"### Response:\\n\")\n",
    "    prediction = prediction[1]\n",
    "    prediction = prediction.split(\"<|end_of_text|>\")\n",
    "    prediction = prediction[0]\n",
    "    sides = prediction.split(\"]\")\n",
    "    prediction = sides[0]\n",
    "    if prediction.endswith(\"'\"):\n",
    "        prediction = ast.literal_eval(prediction + \"]\")\n",
    "    else:\n",
    "        prediction = prediction + \"']\"\n",
    "        prediction = ast.literal_eval(prediction)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702948c0",
   "metadata": {},
   "source": [
    "# Aspect/Laptops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67811a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference with laptops_aspect_model/laptops_aspect_tokenizer/laptops_aspect_instruction_prompt\n",
    "def get_aspect_laptops(user_description):\n",
    "    input_text = user_description\n",
    "    inputs = laptops_aspect_tokenizer(\n",
    "    [\n",
    "      alpaca_prompt.format(\n",
    "          laptops_aspect_instruction_prompt,\n",
    "          input_text, # input\n",
    "          \"\", # output - leave this blank for generation!\n",
    "      )\n",
    "    ], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = laptops_aspect_model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "    output = laptops_aspect_tokenizer.batch_decode(outputs)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Preprocessing the model output\n",
    "def get_aspect_laptops_list(output):\n",
    "    \n",
    "    prediction = output\n",
    "    # prediction = ast.literal_eval(prediction)\n",
    "    prediction = prediction[0]\n",
    "    prediction = prediction.split(\"### Response:\\n\")\n",
    "    prediction = prediction[1]\n",
    "    prediction = prediction.split(\"<|end_of_text|>\")\n",
    "    prediction = prediction[0]\n",
    "    sides = prediction.split(\"]\")\n",
    "    prediction = sides[0]\n",
    "    if prediction.endswith(\"'\"):\n",
    "        prediction = ast.literal_eval(prediction + \"]\")\n",
    "    else:\n",
    "        prediction = prediction + \"']\"\n",
    "        prediction = ast.literal_eval(prediction)\n",
    "        \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e84d2af",
   "metadata": {},
   "source": [
    "# Sentiment/Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c19e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get users prediction.\n",
    "def get_aspect_sentiment_restaurants(output, aspect):\n",
    "    aspect_sentiment_list = []\n",
    "    predictions_list = []\n",
    "\n",
    "    prediction = output\n",
    "    prediction = prediction[0]\n",
    "    prediction = prediction.split(\"### Response:\\n\")\n",
    "    prediction = prediction[1]\n",
    "    prediction = prediction.split(\"<|end_of_text|>\")\n",
    "    prediction = prediction[0]\n",
    "\n",
    "    if \",\" in prediction:\n",
    "        predictions = prediction.split(\",\")\n",
    "        for prediction in predictions:\n",
    "            predictions_list.append(prediction)\n",
    "    else:\n",
    "        predictions_list.append(prediction)\n",
    "        \n",
    "    for i in range(len(predictions_list)):\n",
    "        aspect_sentiment_prediction = (aspect[i], predictions_list[i])\n",
    "        aspect_sentiment_list.append(aspect_sentiment_prediction)\n",
    "        \n",
    "    return aspect_sentiment_list\n",
    "        \n",
    "\n",
    "# Inference with restaurants_sentiment_model/restaurants_sentiment_tokenizer/restaurants_sentiment_instruction_prompt\n",
    "def get_sentiment_restaurants(user_description, restaurants_aspects_list):\n",
    "    sentence = user_description\n",
    "    aspect = restaurants_aspects_list\n",
    "    \n",
    "    input_text = f\"The sentence is: \\\"{sentence}\\\" and the Aspect is: \\\"{aspect}\\\"\"\n",
    "\n",
    "    inputs = restaurants_sentiment_tokenizer(\n",
    "    [\n",
    "      alpaca_prompt.format(\n",
    "          restaurants_sentiment_instruction_prompt,\n",
    "          input_text, # input\n",
    "          \"\", # output - leave this blank for generation!\n",
    "      )\n",
    "    ], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = restaurants_sentiment_model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "    output = restaurants_sentiment_tokenizer.batch_decode(outputs)\n",
    "    \n",
    "    aspect_sentiment_restaurants = get_aspect_sentiment_restaurants(output, restaurants_aspects_list)\n",
    "    \n",
    "    return aspect_sentiment_restaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba34a348",
   "metadata": {},
   "source": [
    "# Sentiment/Laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e69704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get users prediction.\n",
    "def get_aspect_sentiment_laptops(output, aspect):\n",
    "    aspect_sentiment_list = []\n",
    "    predictions_list = []\n",
    "\n",
    "    prediction = output\n",
    "    prediction = prediction[0]\n",
    "    prediction = prediction.split(\"### Response:\\n\")\n",
    "    prediction = prediction[1]\n",
    "    prediction = prediction.split(\"<|end_of_text|>\")\n",
    "    prediction = prediction[0]\n",
    "\n",
    "    if \",\" in prediction:\n",
    "        predictions = prediction.split(\",\")\n",
    "        for prediction in predictions:\n",
    "            predictions_list.append(prediction)\n",
    "    else:\n",
    "        predictions_list.append(prediction)\n",
    "\n",
    "    for i in range(len(predictions_list)):\n",
    "        aspect_sentiment_prediction = (aspect[i], predictions_list[i])\n",
    "        aspect_sentiment_list.append(aspect_sentiment_prediction)\n",
    "\n",
    "    return aspect_sentiment_list\n",
    "    \n",
    "    \n",
    "    \n",
    "# Inference with laptops_sentiment_model/laptops_sentiment_tokenizer/laptops_sentiment_instruction_prompt\n",
    "def get_sentiment_laptops(user_description, laptops_aspects_list):\n",
    "    \n",
    "    sentence = user_description\n",
    "    aspect = laptops_aspects_list\n",
    "    \n",
    "    input_text = f\"The sentence is: \\\"{sentence}\\\" and the Aspect is: \\\"{aspect}\\\"\"\n",
    "\n",
    "    inputs = laptops_sentiment_tokenizer(\n",
    "    [\n",
    "      alpaca_prompt.format(\n",
    "          laptops_sentiment_instruction_prompt,\n",
    "          input_text, # input\n",
    "          \"\", # output - leave this blank for generation!\n",
    "      )\n",
    "    ], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = laptops_sentiment_model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "    output = laptops_sentiment_tokenizer.batch_decode(outputs)\n",
    "\n",
    "    # output = pre_process_output(output)\n",
    "    aspect_sentiment_laptops = get_aspect_sentiment_restaurants(output, aspect)\n",
    "    \n",
    "    return aspect_sentiment_laptops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344cbb82",
   "metadata": {},
   "source": [
    "# Now we defined the functon for our APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235326fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_aspects(domain,model, user_description):\n",
    "    \n",
    "    if domain == \"restaurants\":\n",
    "        output = get_aspect_restaurants(user_description)\n",
    "        \n",
    "        list_aspects = get_aspect_restaurants_list(output)\n",
    "    else:\n",
    "        output = get_aspect_laptops(user_description)\n",
    "        \n",
    "        list_aspects = get_aspect_laptops_list(output)\n",
    "        \n",
    "    return list_aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79d334d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cacca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_sentiment_restaurants(user_description, restaurants_aspects_list)\n",
    "\n",
    "def get_sentiments(domain, model, user_description, list_aspects):\n",
    "    \n",
    "    if domain == \"restaurants\":\n",
    "        aspect_sentiment_list = get_sentiment_restaurants(user_description, list_aspects)\n",
    "    else:\n",
    "        aspect_sentiment_list = get_sentiment_laptops(user_description, list_aspects)\n",
    "        \n",
    "    return aspect_sentiment_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365e5f70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40e1abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/get_aspects', methods=['POST'])\n",
    "def get_aspects_api():\n",
    "    \n",
    "    data = request.get_json()\n",
    "\n",
    "    domain = data.get('domain', None)\n",
    "    model = data.get('model', None)\n",
    "    user_description = data.get('user_description', None)\n",
    "    \n",
    "    list_aspects = generate_aspects(domain,model, user_description)\n",
    "    \n",
    "    list_aspects_sentiments = get_sentiments(domain,model, user_description, list_aspects)\n",
    "    \n",
    "    return_message = {\n",
    "        \"list_aspects_sentiments\": list_aspects_sentiments\n",
    "    }\n",
    "    \n",
    "    return jsonify(return_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00347daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5200\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [31/Aug/2024 19:58:57] \"OPTIONS /get_aspects HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [31/Aug/2024 19:58:58] \"POST /get_aspects HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspect_laptop_mistral_7b\n",
      "aspect_laptop_mistral_7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [31/Aug/2024 20:00:14] \"OPTIONS /get_aspects HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [31/Aug/2024 20:00:14] \"POST /get_aspects HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspect_laptop_mistral_7b\n",
      "aspect_laptop_mistral_7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [31/Aug/2024 20:00:27] \"OPTIONS /get_aspects HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [31/Aug/2024 20:00:27] \"POST /get_aspects HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspect_laptop_mistral_7b\n",
      "aspect_laptop_mistral_7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [31/Aug/2024 20:00:34] \"OPTIONS /get_aspects HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [31/Aug/2024 20:00:34] \"POST /get_aspects HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspect_restaurants_mistral_7b\n",
      "aspect_restaurants_mistral_7b\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(port=5200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39af542f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02be0af0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d08fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbe368e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0475e440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7408fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "github_env",
   "language": "python",
   "name": "github_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
